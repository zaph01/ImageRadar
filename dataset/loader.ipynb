{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21800443",
   "metadata": {},
   "source": [
    "# Loader Datei für NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024dc16",
   "metadata": {},
   "source": [
    "Loader teilt dataset in einen Train, Validation und Test set auf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d6c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9052d",
   "metadata": {},
   "source": [
    "auswählen welche Val und Test set sein sollen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43dcbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequences = {'Validation':['RECORD@2020-11-22_12.49.56','RECORD@2020-11-22_12.11.49','RECORD@2020-11-22_12.28.47','RECORD@2020-11-21_14.25.06'],\n",
    "            'Test':['RECORD@2020-11-22_12.45.05','RECORD@2020-11-22_12.25.47','RECORD@2020-11-22_12.03.47','RECORD@2020-11-22_12.54.38']}\n",
    "# nach beispiel: ['RECORD@2020-11-22_12.49.56','RECORD@2020-11-22_12.11.49','RECORD@2020-11-22_12.28.47','RECORD@2020-11-21_14.25.06']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b190f8",
   "metadata": {},
   "source": [
    "Klasse um batch zu erstellen  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3c7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RADIal_collate(batch):\n",
    "    images = []                 # Platzhalter definieren \n",
    "    FFTs = []\n",
    "    laser_pcs = []\n",
    "    radar_pcs = []\n",
    "    segmaps = []\n",
    "    labels = []\n",
    "\n",
    "    for image, radar_FFT, radar_pc, laser_pc,segmap,box_labels in batch:\n",
    "        labels.append(torch.from_numpy(box_labels))           # torch.from_numpy  -> gibt einen Tensor zurück (das box_labels array und der\n",
    "                                                              # und der entstandene Tensor teilen sich einen Speicher, größe des Tensors lässt sich ncicht ändern)\n",
    "        images.append(torch.tensor(image))                    # torch.tensor -> kopiert Daten\n",
    "        FFTs.append(torch.tensor(radar_FFT).permute(2,0,1))   # torch.permute  -> gibt Teile des Tensor wieder (Postionen werdne angegeben)\n",
    "        segmaps.append(torch.tensor(segmap))\n",
    "        laser_pcs.append(torch.from_numpy(laser_pc))\n",
    "        radar_pcs.append(torch.from_numpy(radar_pc))\n",
    "        \n",
    "    return torch.stack(images), torch.stack(FFTs), torch.stack(segmaps),laser_pcs,radar_pcs,labels      # torch.stack  -> verkettet Tensoren entlang einer neuen Dimension \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ef8d8",
   "metadata": {},
   "source": [
    "Klasse um Loader zu erstellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f18b98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoaders(dataset,batch_size=4,num_workers=2,seed=0):   # batch_size -> Datenloader lädt Daten in Batches mit angegebener Größe\n",
    "                                                                    # num_workers -> Anzahl Prozesse, die zum Laden der Daten verwendet werden (Ladegescheindigkeit kann erhöht werden)\n",
    "                                                                    # seed -> wichtig für Reproduzierbarkeit der Ergebnisse \n",
    "    dict_index_to_keys = {s:i for i,s in enumerate(dataset.sample_keys)} # jedem dataset.sample_kexs wird ein index zugeordnet \n",
    "\n",
    "    Val_indexes = []                                           # Daten werden aufgeteilt \n",
    "    for seq in Sequences['Validation']:                        # durchlaufen der Daten \n",
    "        idx = np.where(dataset.labels[:,14]==seq)[0]\n",
    "        Val_indexes.append(dataset.labels[idx,0])              # index der Sequenz wird gefunden und Liste hinzugefügt \n",
    "    Val_indexes = np.unique(np.concatenate(Val_indexes))       # dopppelte Sequenzen werden rausgenommen \n",
    "\n",
    "    Test_indexes = []\n",
    "    for seq in Sequences['Test']:\n",
    "        idx = np.where(dataset.labels[:,14]==seq)[0]\n",
    "        Test_indexes.append(dataset.labels[idx,0])\n",
    "    Test_indexes = np.unique(np.concatenate(Test_indexes))\n",
    "\n",
    "    val_ids = [dict_index_to_keys[k] for k in Val_indexes]     # entsprechende Sequenz-IDs werden aus dem Dataset ausgewählt\n",
    "    test_ids = [dict_index_to_keys[k] for k in Test_indexes]\n",
    "    train_ids = np.setdiff1d(np.arange(len(dataset)),np.concatenate([val_ids,test_ids]))   # np.setdiff1d -> findet Unterschiede zwischem den beiden arrays\n",
    "                                                                                           # gibt die werte von array 1 an, die nicht in array 2 vorkommen \n",
    "    train_dataset = Subset(dataset,train_ids)                  \n",
    "    val_dataset = Subset(dataset,val_ids)  \n",
    "    test_dataset = Subset(dataset,test_ids)\n",
    "\n",
    "    # Erstellen der data_loaders (um die Daten in Batches zu laden, zu mischen und zu transformieren, bevor sie in das Modell eingespeist werden)\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=RADIal_collate)\n",
    "    val_loader =  DataLoader(val_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=RADIal_collate)\n",
    "    test_loader =  DataLoader(test_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=RADIal_collate)\n",
    "\n",
    "    return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f621772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
