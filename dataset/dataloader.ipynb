{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870e6259",
   "metadata": {},
   "source": [
    "# Datenloader-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e133e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.transforms import Resize,CenterCrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b697d2",
   "metadata": {},
   "source": [
    " Daten Laden in ein Dataset (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b340afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'E:\\\\Jakob\\\\Entwicklungsprojekt\\\\ungezippt\\\\RADIal-001\\\\RADIal'\n",
    "labels = pd.read_csv(os.path.join(base_dir,'labels.csv'))\n",
    "labels_np = labels.to_numpy()\n",
    "\n",
    "# um nur einfache Werte difficult = 1 anzuzeigen\n",
    "ids_filters=[]\n",
    "ids = np.where(labels_np[:, -1] == 0)[0]\n",
    "ids_filters.append(ids)\n",
    "ids_filters = np.unique(np.concatenate(ids_filters))\n",
    "labels_np = labels_np[ids_filters]\n",
    "\n",
    "# um nur noch eindeutige numSamples in 1. Spalte zu haben \n",
    "unique_ids = np.unique(labels_np[:,0])\n",
    "label_dict = {}\n",
    "for i,ids in enumerate(unique_ids):\n",
    "    sample_ids = np.where(labels_np[:,0]==ids)[0]\n",
    "    label_dict[ids]=sample_ids\n",
    "sample_keys = list(label_dict.keys())\n",
    "\n",
    "len_dict = len(label_dict)\n",
    "\n",
    "dataset_RPC = []\n",
    "box_labels = []\n",
    "for i in range(3000):\n",
    "    sample_id = sample_keys[i] \n",
    "    # From the sample id, retrieve all the labels ids\n",
    "    entries_indexes = label_dict[sample_id]\n",
    "    # Get the objects labels\n",
    "    box_labels_one = labels_np[entries_indexes]\n",
    "\n",
    "    # Labels contains following parameters:\n",
    "    # numSample\tx1_pix\ty1_pix\tx2_pix\ty2_pix\tlaser_X_m\tlaser_Y_m\tlaser_Z_m\tradar_X_m\tradar_Y_m\tradar_R_m\n",
    "    # radar_A_deg\tradar_D_mps\tradar_P_db\n",
    "    box_labels_one = box_labels_one[:,10:13].astype(np.float32)\n",
    "    box_labels.append(box_labels_one)  # Ergebnisse zur Liste hinzufügen\n",
    "    RPC_filename = os.path.join(base_dir,'radar_PCL',\"pcl_{:06d}.npy\".format(sample_id))\n",
    "    # range,azimuth,elevation,power,doppler,x,y,z,v  ---> [0,1,4]\n",
    "    dataset_RPC.append(np.load(RPC_filename,allow_pickle=True)[[0,1,4],:])\n",
    "    \n",
    "    df = pd.DataFrame({'Data_RPC': dataset_RPC, 'Labels': box_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76339003",
   "metadata": {},
   "source": [
    "Klasse zum erstellen von Batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9459386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RADIal_collate(batch):\n",
    "    radar_pcs = []\n",
    "    labels = []\n",
    "\n",
    "    for dataset_RPC, box_labels in batch:\n",
    "        labels.append(torch.from_numpy(box_labels))           # torch.from_numpy  -> gibt einen Tensor zurück (das box_labels array und der\n",
    "                                                              # und der entstandene Tensor teilen sich einen Speicher, größe des Tensors lässt sich ncicht ändern)\n",
    "        radar_pcs.append(torch.from_numpy(dataset_RPC))\n",
    "        \n",
    "    return radar_pcs,labels            # torch.stack  -> verkettet Tensoren entlang einer neuen Dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58bc63",
   "metadata": {},
   "source": [
    "Klasse zur Loader Erstellung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9ab9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoaders(dataset,batch_size=4,shuffle=True,num_workers=0,seed=0):   # batch_size -> Datenloader lädt Daten in Batches mit angegebener Größe\n",
    "                                                                    # num_workers -> Anzahl Prozesse, die zum Laden der Daten verwendet werden (Ladegescheindigkeit kann erhöht werden)\n",
    "                                                                    # seed -> wichtig für Reproduzierbarkeit der Ergebnisse \n",
    "                                                                    # shuffle -> zum Durchmische des datasets\n",
    "    dict_index_to_keys = {s:i for i,s in enumerate(sample_keys)}    # jedem dataset.sample_kexs wird ein index zugeordnet \n",
    "    \n",
    "    Test_indexes = []                                 # Daten werden aufgeteilt \n",
    "    for i in range(500):                              # durchlaufen der Daten \n",
    "        Test_indexes.append(box_labels[i])            # index der Sequenz wird gefunden und Liste hinzugefügt \n",
    "   \n",
    "    Train_indexes = []\n",
    "    for i in range(500, 2500):\n",
    "        Train_indexes.append(box_labels[i])\n",
    "\n",
    "                                                                                         # gibt die werte von array 1 an, die nicht in array 2 vorkommen \n",
    "    train_dataset = Subset(dataset,Train_indexes)                  \n",
    "    test_dataset = Subset(dataset,Test_indexes)\n",
    "\n",
    "    # Erstellen der data_loaders (um die Daten in Batches zu laden, zu mischen und zu transformieren, bevor sie in das Modell eingespeist werden)\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=RADIal_collate)\n",
    "\n",
    "    test_loader =  DataLoader(test_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True,\n",
    "                            collate_fn=RADIal_collate)\n",
    " \n",
    "    return train_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2583f3",
   "metadata": {},
   "source": [
    "aufruf der Loader-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bc230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = CreateDataLoaders(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d19ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
